{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0,\"./..\")\n",
    "from src.PassivePy import PassivePyAnalyzer\n",
    "from src import analysis_passive\n",
    "\n",
    "#analyzer = PassivePyAnalyzer(spacy_model= \"fr_core_news_lg\")\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/robin/Code_repo/app_passive_detection/data/intransif_verbs.txt') as f : \n",
    "   lines =  f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [elt.replace(\"\\n\",\"\").lower().strip() for elt in  open('/home/robin/Code_repo/app_passive_detection/data/intransif_verbs.txt').readlines()[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2299"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/env/app_passive/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 10:10:53.525 INFO    stanza: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 6.90MB/s]                    \n",
      "2022-12-07 10:10:55.896 INFO    stanza: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2022-12-07 10:10:55.898 INFO    stanza: Use device: gpu\n",
      "2022-12-07 10:10:55.899 INFO    stanza: Loading: tokenize\n",
      "2022-12-07 10:10:55.915 INFO    stanza: Loading: mwt\n",
      "2022-12-07 10:10:55.923 INFO    stanza: Loading: pos\n",
      "2022-12-07 10:10:56.363 INFO    stanza: Loading: lemma\n",
      "2022-12-07 10:10:56.427 INFO    stanza: Loading: depparse\n",
      "2022-12-07 10:10:56.734 INFO    stanza: Loading: ner\n",
      "2022-12-07 10:10:58.495 INFO    stanza: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy_stanza.load_pipeline(\"fr\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/env/app_passive/lib/python3.8/site-packages/spacy/language.py:1014: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  doc = self._ensure_doc(text)\n",
      "/home/robin/env/app_passive/lib/python3.8/site-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['Spiderman', 'a', 'été', 'vu', 'à', 'le', 'cinéma', '.', 'La', 'pomme', 'a', 'été', 'posée', 'sur', 'la', 'table', '!', \"J'\", 'ai', 'fais', 'à', 'manger', 'des', 'pattes', 'à', 'mes', 'amis', '.']\n",
      "Entities: [('Spiderman', 'MISC', 0, 9), ('La pomme', 'MISC', 30, 38)]\n",
      "  doc = self._ensure_doc(text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[vu, posée, fais, manger]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc  = nlp(\"Spiderman a été vu au cinéma. La pomme a été posée sur la table ! J'ai fais à manger des pattes à mes amis.\")\n",
    "totalVerbs = [token for token in doc if (token.pos_ == \"VERB\")]\n",
    "totalVerbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vu\n",
      "nsubj:pass\n",
      "aux:tense\n",
      "aux:pass\n",
      "obl:arg\n",
      "punct\n",
      "posée\n",
      "nsubj:pass\n",
      "aux:tense\n",
      "aux:pass\n",
      "obl:mod\n",
      "punct\n",
      "fais\n",
      "nsubj\n",
      "aux:tense\n",
      "xcomp\n",
      "punct\n",
      "manger\n",
      "mark\n",
      "obj\n",
      "obl:mod\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for verb in totalVerbs:\n",
    "    # direct object check\n",
    "    directObject = False\n",
    "    # indirect object check\n",
    "    indirectObject = False\n",
    "    \n",
    "    \n",
    "    print(verb)\n",
    "    # looks through each verb's child ie dependents\n",
    "    for item in verb.children:\n",
    "    \n",
    "        print(item.dep_)\n",
    "        # sets indirectObject to true if indirect object is found\n",
    "        if(item.dep_ == \"iobj\" or item.dep_ == \"pobj\"):\n",
    "            indirectObject = True\n",
    "        # sets directObject to true if direct object is found\n",
    "        if (item.dep_ == \"dobj\" or item.dep_ == \"dative\"):\n",
    "            directObject = True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "r = [\n",
    "        {\"TAG\":\"ADJ\", \"TEXT\" : {\"REGEX\": \"\\b(\\w*(ible|able))\\b\"}}, # adjectif se finnissant par ible ou able\n",
    "    ]\n",
    "matcher.add(\"test\", [r], greedy='LONGEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1618900948208871284, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "r = [\n",
    "        {\"TAG\":\"ADJ\",\"TEXT\" : {\"REGEX\": r\"\\b(\\w*(ible|able))\\b\"}}, # adjectif se finnissant par ible ou able\n",
    "    ]\n",
    "matcher.add(\"test\", [r], greedy='LONGEST')\n",
    "\n",
    "doc = nlp(\"cette ile est visible depuis l'ocean\")\n",
    "for match in matcher(doc):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] =  [\"elle a été déterminée et formée ! \",\n",
    "\"accusé par la police, il fut condamné a 5 ans de prison ferme\"\n",
    ",\"ma mère a été élevée par une euh une une vieille dame euh à euh dans l'Allier\"\n",
    ",\"puis on a été soutenu par un monsieur aussi c'est monsieur NPERS…\",\n",
    "\"la la République du Centre a été rachetée euh par un journal euh je voudrais pas dire de bêtises je crois que c'est Clermont-Ferrand Clermont enfin…\",\n",
    "\"alors il y a des écoles qui sont faites exclusivement par des religieuses ou certains par des prêtres mais alors maintenant ils ont beaucoup d'institutrices euh civiles qui sont dedans\",\n",
    "\"donc il est il est payé quoi bah c'est super\",\n",
    "\"à à mon avis ça a déjà été fait mais au niveau euh professionnel au niveau du travail je crois que ça se développe ouais\",\n",
    "\"quand tu habites dans le centre d'Orléans euh c'est quand même réservé à un public qui a … un minimum d'argent\",\n",
    "\"parce que même avant on a été coincé euh pendant pas mal d'années euh sans voiture et on travaillait pas\",\"tu es à peine payé et tout quoi\",\n",
    "\"tout à l'heure j'avais été inter- la dernière fois que j'avais été interviewée c'était pour la la construction de la nouvelle fac\",\n",
    "\"donc vous êtes  envoyée ponctuellement sur des sur des sur des sites sites\",\n",
    "\"d'accord je devais pas rester à Orléans je devais juste être formée pendant un an et partir sur Lille\",\n",
    "\"je trouve que pour le moment euh c'est pas encore trop bien aménagé euh pour les vélos hein vélos\",\n",
    "\"Les adultes quand tu habites dans le centre d'Orléans euh c'est quand même réservé à un public qui a hm ouais ouais bien sûr ouais un minimum d'argent\"\n",
    "\"des  des échanges qui qui se font pas par téléphone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting Sentences...\n",
      "Starting to find passives...\n"
     ]
    }
   ],
   "source": [
    "data, cols = analysis_passive.load_passive_features(data, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "été attaqué\n",
      "rule:  passif_tronqué\n",
      "été attaqué par\n",
      "rule:  passif_canonique\n",
      "attaqué par des loups\n",
      "rule:  passif_impersonel\n"
     ]
    }
   ],
   "source": [
    "text = \"J'ai été attaqué par des loups !\"\n",
    "samples = analyzer.print_matches(text)\n",
    "doc = analyzer.prepare_visualisation(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy_streamlit\u001b[39;00m \u001b[39mimport\u001b[39;00m visualize_spans\n\u001b[0;32m----> 2\u001b[0m matches, doc \u001b[39m=\u001b[39m analyzer\u001b[39m.\u001b[39mprepare_visualisation(text)\n\u001b[1;32m      3\u001b[0m visualize_spans(doc, spans_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpassive\u001b[39m\u001b[39m\"\u001b[39m, show_table \u001b[39m=\u001b[39m  \u001b[39mFalse\u001b[39;00m, title \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyzer' is not defined"
     ]
    }
   ],
   "source": [
    "from spacy_streamlit import visualize_spans\n",
    "matches, doc = analyzer.prepare_visualisation(text)\n",
    "visualize_spans(doc, spans_key=\"passive\", show_table =  False, title = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "doc = nlp(\"je suis un exemple de text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"elle a été déterminée et formée ! La pomme a été mangé par le chat\")\n",
    "ids = [(elt[1],elt[2]) for elt in passivepy.matcher(doc)]\n",
    "samples = []\n",
    "\n",
    "color_dict = {\n",
    "    \"passif_tronqué\": \"#8ef\",\n",
    "    \"passive_rule_2\" : \"#faa\",\n",
    "    \"passif_sequencé\" : \"afa\",\n",
    "    \"passif_impersonel\":\"#fea\",\n",
    "    \"passive_rule_5\":\"#faa\",\n",
    "    \"passif_verbale\":\"#000080\",\n",
    "    \"passif_factif\" : \"#00FF00\",\n",
    "    \"passif_adjectif\":\"#808080\"   \n",
    "}\n",
    "\n",
    "for token in doc :\n",
    "    for id_, s,e  in passivepy.matcher(doc) :\n",
    "        if not s<token.i<e :\n",
    "            samples.append((token.text))\n",
    "        else :\n",
    "            rule_name = \"test\"#nlp.vocab.strings[id_]\n",
    "            rule_color = \"C\"#color_dict[rule_name]\n",
    "            samples.append((token.text,rule_name, rule_color))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elle a été déterminée et formée ! La pomme a été mangé par le chat'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('app_passive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e13b7e429d104479b6616cf130de1287aa659b19c5e0239a306aa6256a95f5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
